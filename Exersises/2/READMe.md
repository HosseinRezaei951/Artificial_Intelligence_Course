# Exercises on RMSE Calculation and Polynomial Feature Expansion

This repository contains two exercises that demonstrate the implementation of the Root Mean Squared Error (RMSE) calculation and the use of polynomial feature expansion in a logistic regression model. These exercises provide insights into model evaluation and the impact of feature engineering on model performance, particularly in the context of overfitting.

## Exercise 1: RMSE Calculation and Analysis (`Solved_3.py`)

This exercise focuses on implementing the RMSE (Root Mean Squared Error) formula in the provided Python code. The code uses a logistic regression model to classify data generated by the `make_moons` function from Scikit-learn. The exercise involves analyzing how the RMSE changes with varying sizes of the test dataset.

### Key Features

- **Data Generation**: Generates a 2D classification dataset using `make_moons`.
- **Training and Testing Split**: Splits the dataset into training and testing sets.
- **Logistic Regression**: Implements logistic regression with gradient descent to fit the model.
- **RMSE Calculation**: Introduces RMSE calculation to evaluate model performance.
- **Analysis**: Observes the impact of increasing test data size on RMSE.

### Observations

- As the test dataset size increases, the RMSE also tends to increase. This could be due to the model being tested more rigorously or the reduction in the training dataset size, leading to a less robust model.
- The exercise highlights the trade-off between the size of training and test datasets and its impact on model performance.

### How to Use

1. Ensure you have the required libraries installed: `numpy`, `matplotlib`, `scikit-learn`, `pandas`.
2. Run the `Solved_3.py` script to execute the logistic regression model and calculate RMSE.
3. Observe the scatter plots generated to visualize the classification and test data, and check the printed RMSE values.

## Exercise 2: Polynomial Feature Expansion and Overfitting Analysis (`Solved_3withPHI.py`)

This exercise explores the effect of polynomial feature expansion on a logistic regression model. It examines how increasing the complexity of features (by adding polynomial terms) influences model performance, particularly in terms of overfitting.

### Key Features

- **Polynomial Feature Expansion**: Expands the features in the training and test datasets by adding polynomial terms up to the 7th degree.
- **Logistic Regression with Polynomial Features**: Implements logistic regression using the expanded feature set.
- **Overfitting Analysis**: Studies how increasing the complexity of the model leads to better fit initially, but eventually causes overfitting.

### Observations

- As polynomial features are added, the model initially improves, fitting the data better. However, beyond a certain degree of complexity, the model starts to overfit, capturing noise rather than the underlying pattern.
- This exercise demonstrates the concept of overfitting, where a model becomes too complex and performs well on training data but poorly on unseen test data.

### How to Use

1. Ensure you have the required libraries installed: `numpy`, `matplotlib`, `scikit-learn`, `pandas`.
2. Run the `Solved_3withPHI.py` script to execute the logistic regression model with polynomial feature expansion.
3. Examine the generated plots to observe how the model fits the data with increasing polynomial degrees and evaluate the accuracy printed by the script.

## Conclusion

These exercises provide hands-on experience with evaluating machine learning models using RMSE and the effects of polynomial feature expansion. By exploring the impact of dataset size and feature complexity, these exercises underscore the importance of careful model evaluation and the dangers of overfitting in machine learning.
